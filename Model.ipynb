{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Import Required Libraries**\n",
                "\n",
                "---\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "from collections import defaultdict\n",
                "import statistics\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.feature_selection import mutual_info_classif\n",
                "from sklearn import tree\n",
                "from sklearn.datasets import load_iris\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.metrics import confusion_matrix\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns; sns.set_theme()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Intialisation of Data**\n",
                "\n",
                "---\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Creating a dictionary of the path to reach the data sets within the assignment 2 folder\n",
                "files = defaultdict(int)\n",
                "for data_types in os.listdir(\"/course/data/a2/\"):\n",
                "    for file_name in os.listdir(\"/course/data/a2/\" + data_types):\n",
                "        name = file_name.split(\".\")[0]\n",
                "        if name == 'description':\n",
                "            name = data_types + \"_\" + name\n",
                "        files[name] = \"/course/data/a2/\" + data_types + \"/\" + file_name\n",
                "\n",
                "# Read and save all raw data\n",
                "KRmatch_raw = pd.read_csv(files['KRmatch'])\n",
                "EUmatch_raw = pd.read_csv(files['EUmatch'])\n",
                "NAmatch_raw = pd.read_csv(files['NAmatch'])\n",
                "\n",
                "# Open and print game description\n",
                "desc_game = open(files['games_description'], \"r\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Helper Functions**\n",
                "\n",
                "---\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "A helper function that extracts the columns from the data, removing any \n",
                "missing value rows, and alphabetically sorting each champion while \n",
                "reseting the index. It also creates new useful columns from the\n",
                "previous columns so that the data is more easily used.\n",
                "\"\"\"\n",
                "\n",
                "def cleaner(dataset):\n",
                "    \n",
                "    columns = ['champion', 'assists', 'damage_objectives', 'damage_building', 'damage_turrets', \n",
                "    'deaths', 'kills', 'time_cc', 'damage_taken', 'vision_score', 'damage_total', 'role', 'gold_earned']\n",
                "    dataset = dataset[columns] # Takes necessary columns\n",
                "    dataset = dataset.dropna() # Removes rows with missing values\n",
                "    dataset = dataset.sort_values(by='champion') # Alphabetically sorted by champion\n",
                "    dataset = dataset.reset_index() # Resets the index\n",
                "    dataset = dataset[columns] \n",
                "\n",
                "    # Creating new columns Kill-to-Death Ratio and Assist-to-Death Ratio\n",
                "    KD = []\n",
                "    AD = []\n",
                "    for row_index in range(0, len(dataset)):\n",
                "        row = dataset.loc[row_index]\n",
                "        deaths = row['deaths']\n",
                "\n",
                "        if deaths == 0:\n",
                "            deaths = 1\n",
                "\n",
                "        row_KD = row['kills'] / deaths\n",
                "        row_AD = row['assists'] / deaths\n",
                "        KD.append(row_KD)\n",
                "        AD.append(row_AD)\n",
                "\n",
                "    dataset['KD'] = KD\n",
                "    dataset['AD'] = AD\n",
                "\n",
                "    return dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "A helper function that uses classification to predict the \n",
                "role of a champion, and returns the average of all the accuracies \n",
                "found, the model, accuracy of the iteration and the confustion matrix for this model\n",
                "\"\"\"\n",
                "\n",
                "def the_classifier(df_with_roles):\n",
                "    conf_matrix = np.array(([0,0],[0,0]))\n",
                "    # Splitting the data frame into all the columns but \"ROLE\"\n",
                "    X = df_with_roles.iloc[:,:-1]\n",
                "    # The ROLE column from the Data Frame\n",
                "    y = df_with_roles.iloc[:,-1]\n",
                "\n",
                "    # Creating a decision tree and performing kfold with 5 splits\n",
                "    dtree = DecisionTreeClassifier()\n",
                "    kfold = KFold(n_splits=5, random_state=None)\n",
                "    accuracy_val = []\n",
                "\n",
                "    # Repeated Kfold 10 times \n",
                "    for R in range(10):\n",
                "        for train_index , test_index in kfold.split(X):\n",
                "            X_train , X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
                "            y_train , y_test = y[train_index] , y[test_index]\n",
                "            \n",
                "            # Performing classification and determining the accuracy\n",
                "            dtree.fit(X_train,y_train)\n",
                "            pred_values = dtree.predict(X_test)\n",
                "            current_accuracy = accuracy_score(pred_values, y_test)\n",
                "            accuracy_val.append(current_accuracy)\n",
                "            cm = confusion_matrix(y_test, pred_values)\n",
                "            conf_matrix += cm\n",
                "        one_acc = current_accuracy\n",
                "\n",
                "    return sum(accuracy_val)/len(accuracy_val), dtree, one_acc, conf_matrix/10"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Main Code**\n",
                "\n",
                "---\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Data Wrangling**\n",
                "\n",
                "---\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Collates each region data into one global dataset\n",
                "KRmatch = cleaner(KRmatch_raw)\n",
                "EUmatch = cleaner(EUmatch_raw)\n",
                "NAmatch = cleaner(NAmatch_raw)\n",
                "ALLmatch = pd.concat([KRmatch, EUmatch, NAmatch])\n",
                "\n",
                "ALLmatch = ALLmatch.drop(\"deaths\", axis = 1)\n",
                "\n",
                "# Discretising the role column and placing it in a list\n",
                "roles = {'TopLane_Jungle' : 1, 'Other' : 0}\n",
                "ALLmatch.role = [roles[item] for item in ALLmatch.role]\n",
                "\n",
                "# Changing the index to the name of the champion\n",
                "Champs = defaultdict(list)\n",
                "ALLmatch = ALLmatch.to_dict('records')\n",
                "for row in ALLmatch:\n",
                "    Champs[row.pop('champion', None)].append(row)\n",
                "\n",
                "# List to store TopLane/Jungle champions\n",
                "TopLane_Jungle_Champs = []\n",
                "\n",
                "\"\"\"\n",
                "Iterating through the champion dictionary, and taking the median \n",
                "and SD of all champions in order to create a DataFrame of all the \n",
                "champions median and Standard Deviation scores.\n",
                "\"\"\"\n",
                "\n",
                "champion_median = defaultdict()\n",
                "\n",
                "for champion in Champs.keys():\n",
                "    champion_df = pd.DataFrame(Champs[champion])\n",
                "    champdf_median = champion_df.median(numeric_only=True)\n",
                "    if champdf_median['role'] \u003e 0:\n",
                "        TopLane_Jungle_Champs.append(champion)\n",
                "\n",
                "    champion_median[champion] = champdf_median.to_dict()\n",
                "\n",
                "# Transposing the data frame\n",
                "champion_sd = pd.DataFrame(champion_median)\n",
                "champion_sd_T = champion_sd.T\n",
                "\n",
                "standardized_df = (champion_sd_T-champion_sd_T.mean())/champion_sd_T.std()\n",
                "standardized_df = standardized_df\n",
                "\n",
                "# Discretising the role and placing it in the Data Frame\n",
                "role_bin = []\n",
                "for row in standardized_df['role']:\n",
                "    if row \u003e 0:\n",
                "        role_bin.append(1)\n",
                "    else:\n",
                "        role_bin.append(0)\n",
                "standardized_df = standardized_df.drop('role', axis = 1)\n",
                "\n",
                "standardized_df['ROLE'] = role_bin"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Modelling**\n",
                "\n",
                "---\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Calculating the mutual information to measure the correlation between features and TopLane_Jungle to\n",
                "determine which features negatively impact the model training and remove them \n",
                "\n",
                "As MI produces a positive value we need to determine a threshold where if a feature is greater than\n",
                "the threshold it is kept and if its below, it's removed. \n",
                "\n",
                "In order to determine what the threshold should be we create a range of values and calculate the \n",
                "accuracy of each classification model and choose the highest accuracy and hence the affiliated \n",
                "threshold."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\"\"\"\n",
                "Determining which MI threshold will produce the highest accuracy\n",
                "in the classification model.\n",
                "\"\"\"\n",
                "\n",
                "mi_arr = mutual_info_classif(X=standardized_df, y=role_bin, discrete_features=False)\n",
                "MI_VAL = defaultdict(int)\n",
                "\n",
                "k_ten = []\n",
                "k_one = []\n",
                "Threshold_used = []\n",
                "Models = [\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\", \"Model 5\", \"Best Model\"]\n",
                "\n",
                "# Testing a range of values for the threshold\n",
                "for THRESHOLD in np.arange(0,0.02, 0.004):\n",
                "    filtered_features = [] \n",
                "\n",
                "    #This formatting of the code was taken from the Week 8 Workshop\n",
                "    # Determining which features' MI values are higher than the THRESHOLD \n",
                "    for feature, mi in zip(standardized_df.columns, mi_arr):\n",
                "        MI_VAL[feature] = round(mi, 5)    \n",
                "        if(mi \u003e= THRESHOLD): \n",
                "            filtered_features.append(feature)\n",
                "    temp_df = standardized_df[filtered_features]\n",
                "\n",
                "    # Calling the classification model to determine accuracy\n",
                "    curr_accuracy, model, one_acc, conf_matrix = the_classifier(standardized_df[filtered_features])\n",
                "\n",
                "    k_ten.append(curr_accuracy)\n",
                "    k_one.append(one_acc)\n",
                "    Threshold_used.append(THRESHOLD)\n",
                "    \n",
                "    # Checking if it's the first iteration\n",
                "    if THRESHOLD == 0:\n",
                "        best_accuracy = curr_accuracy\n",
                "\n",
                "    # Checking for the best accuracy and performing\n",
                "    # Feature selection \n",
                "    if curr_accuracy \u003e best_accuracy:\n",
                "        best_accuracy = curr_accuracy\n",
                "        BEST_FEATURES = filtered_features\n",
                "        Best_Thresh = THRESHOLD\n",
                "\n",
                "\n",
                "MI_VAL.pop(\"ROLE\")\n",
                "\n",
                "MI_DF = pd.DataFrame.from_dict(MI_VAL, orient='index', columns=[\"MI Value\"])\n",
                "MI_DF.to_csv('Features.csv', index=True)\n",
                "\n",
                "# Creating the final model with the best features\n",
                "score, Model, one_acc, conf_matrix = the_classifier(standardized_df[BEST_FEATURES])\n",
                "\n",
                "k_ten.append(score)\n",
                "k_one.append(one_acc)\n",
                "Threshold_used.append(Best_Thresh)\n",
                "\n",
                "Accuracy_DF = pd.DataFrame([Threshold_used, k_one, k_ten], index=[\"Threshold Used\", \"1 - Fold Accuracy Score\", \"10 - Fold Accuracy Score\" ]).T\n",
                "Accuracy_DF[\"Models\"] = Models\n",
                "Accuracy_DF = Accuracy_DF.set_index('Models').round(5)\n",
                "Accuracy_DF.to_csv('Accuracy.csv', index=True)\n",
                "score = round(score, 5)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Plotting Data**\n",
                "\n",
                "---\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Graph Decison Tree\n",
                "fig = plt.figure(figsize=(25,20))\n",
                "tree.plot_tree(Model)\n",
                "plt.title(\"Decision Tree Classifer Model: 10-Fold Classification\", size=32)\n",
                "plt.savefig(\"ModelTree.png\")\n",
                "plt.close() #UNCOMMENT TO HIDE GRAPH\n",
                "\n",
                "# Graph Confusion Matrix\n",
                "conf_matrix_df = pd.DataFrame(conf_matrix, index = [i for i in [\"Predicted Positive\", \"Predicted Negative\"]],\n",
                "                  columns = [i for i in [\"Actual Positive\", \"Actual Negative\"]])\n",
                "plt.figure()\n",
                "plt.title(\"Concacted Confusion Matrix: 10-Fold Classification\")\n",
                "sns.heatmap(conf_matrix_df, annot=True)\n",
                "plt.autoscale()\n",
                "plt.savefig(\"confusion_matrix.png\")\n",
                "plt.close() #UNCOMMENT TO HIDE GRAPH\n",
                "\n",
                "# Graph Density Functions\n",
                "Features = BEST_FEATURES[:-1]\n",
                "Only_JGL = standardized_df.loc[standardized_df[\"ROLE\"] \u003e 0]\n",
                "for feat in Features:\n",
                "    CURR_DF = Only_JGL[feat]\n",
                "    plt.figure()\n",
                "    CURR_DF.plot.density(color='green')\n",
                "    plt.title('Density plot for ' + feat)\n",
                "    plt.ylabel(\"Probabilty Density Estimation\")\n",
                "    plt.xlabel(\"Standardized Score for \" + feat)\n",
                "    plt.autoscale()\n",
                "    plt.savefig(\"graphs/\" + feat + \"-graph.png\")\n",
                "    plt.close()    #UNCOMMENT TO HIDE GRAPHS"
            ]
        }
    ]
}
